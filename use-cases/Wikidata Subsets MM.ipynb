{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Subsets of Wikidata\n",
    "\n",
    ">Warning: \n",
    "**This notebook is under construction and it doesn't work**\n",
    "\n",
    "## Purpose\n",
    "\n",
    ">This notebook is used to create smaller subgraphs from a larger input Wikidata graph. Notebook users can provide a list of Wikidata classes (**QNodes**) to remove and preserve to create desired subsets of Wikidata. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Invocation\n",
    "Example batch command. The second argument is a notebook where the output will be stored. You can load it to see progress.\n",
    "\n",
    "UPDATE EXAMPLE INVOCATION\n",
    "\n",
    "\n",
    "```\n",
    "papermill Wikidata\\ Useful\\ Files.ipynb useful-files.out.ipynb \\\n",
    "-p wiki_file /Volumes/GoogleDrive/Shared\\ drives/KGTK-public-graphs/wikidata-20200803-v3/all.tsv.gz \\\n",
    "-p label_file /Volumes/GoogleDrive/Shared\\ drives/KGTK-public-graphs/wikidata-20200803-v3/part.label.en.tsv.gz \\\n",
    "-p item_file /Volumes/GoogleDrive/Shared\\ drives/KGTK-public-graphs/wikidata-20200803-v3/part.wikibase-item.tsv.gz \\\n",
    "-p property_item_file /Volumes/GoogleDrive/Shared\\ drives/KGTK-public-graphs/wikidata-20200803-v3/part.property.wikibase-item.tsv.gz \\\n",
    "-p qual_file /Volumes/GoogleDrive/Shared\\ drives/KGTK-public-graphs/wikidata-20200803-v3/qual.tsv.gz \\\n",
    "-p output_path <local folder> \\\n",
    "-p output_folder useful_files_v4 \\\n",
    "-p temp_folder temp.useful_files_v4 \\\n",
    "-p delete_database no \\\n",
    "-p compute_pagerank no \\\n",
    "-p languages es,ru,zh-cn \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Folder on local machine where to create the output and temporary folders\n",
    "# output_path = \"/Users/pedroszekely/Downloads/kypher\"\n",
    "output_path = \"/Users/markmann/Downloads/subset\"\n",
    "\n",
    "# The names of the output and temporary folders\n",
    "output_folder = \"output\"\n",
    "temp_folder = \"temp.output\"\n",
    "\n",
    "# Classes to remove\n",
    "# remove_classes = \"Q13442814, Q523, Q16521, Q318, Q7318358, Q7187, Q11173, Q8054, Q5, Q13100073, Q8502, Q3305213, Q4022, Q79007, Q1931185, Q30612, Q101352, Q54050, Q13433827, Q2668072, Q23397, Q3863, Q11424, Q482994, Q47150325, Q16970, Q18593264, Q355304, Q9842, Q7725634, Q27020041, Q56436498, Q2154519, Q61443690, Q49008, Q3331189, Q47521, Q5084, Q19389637, Q21014462, Q4164871, Q11060274, Q5633421, Q39816, Q5185279, Q55488, Q134556, Q22698, Q985488, Q1260524, Q204107, Q2225692, Q215380, Q71963409, Q452237, Q93184, Q12323\"\n",
    "\n",
    "# The location of input files\n",
    "# wiki_root_folder = \"/Volumes/GoogleDrive/Shared\\ drives/KGTK/datasets/wikidata-20200803-v4/\"\n",
    "# wiki_root_folder = \"/Volumes/GoogleDrive/Shared\\ drives/KGTK/datasets/wikidata-20200803-v4/\"\n",
    "# wiki_root_folder = \"/Users/pedroszekely/Downloads/kypher/wikidataos-v4/\"\n",
    "wiki_root_folder = \"/Users/markmann/Google\\ Drive\\ File\\ Stream/Shared\\ drives/KGTK/datasets/wikidataos-v4-mm-2/\"\n",
    "\n",
    "claims_file = \"claims.tsv.gz\"\n",
    "label_file = \"labels.en.tsv.gz\"\n",
    "alias_file = \"aliases.en.tsv.gz\"\n",
    "description_file = \"descriptions.en.tsv.gz\"\n",
    "item_file = \"claims.wikibase-item.tsv.gz\"\n",
    "qual_file = \"qualifiers.tsv.gz\"\n",
    "property_datatypes_file = \"metadata.property.datatypes.tsv.gz\"\n",
    "isa_file = \"derived.isa.tsv.gz\"\n",
    "p279star_file = \"derived.P279star.tsv.gz\"\n",
    "\n",
    "# Useful files Jupyter notebook\n",
    "useful_files_notebook = \"Wikidata Useful Files.ipynb\"\n",
    "notebooks_folder = \"/Users/markmann/Desktop/CKG/kgtk_subset/kgtk/examples/\"\n",
    "\n",
    "# Location of the cache database for kypher\n",
    "# cache_path = \"/Users/pedroszekely/Downloads/kypher/wikidataos-v4\"\n",
    "cache_path = f'{output_path}/{output_folder}'\n",
    "\n",
    "#Additional parameters\n",
    "delete_database = \"no\"\n",
    "compute_pagerank = \"no\"\n",
    "languages = \"\"\n",
    "\n",
    "### Needs fixing\n",
    "# Whether to delete the cache database\n",
    "if delete_database and delete_database.lower().strip() == 'yes':\n",
    "    delete_database = True\n",
    "else:\n",
    "    delete_database = False\n",
    "\n",
    "### Needs fixing\n",
    "if compute_pagerank and compute_pagerank.lower().strip() == 'yes':\n",
    "    compute_pagerank = True\n",
    "else:\n",
    "    compute_pagerank = False\n",
    "\n",
    "if languages:\n",
    "    languages = languages.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import papermill as pm\n",
    "import gzip\n",
    "\n",
    "import gzip\n",
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "from utils import Remove_Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up variables for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment variables\n",
    "if cache_path:\n",
    "    os.environ['STORE'] = \"{}/wikidata.sqlite3.db\".format(cache_path)\n",
    "else:\n",
    "    os.environ['STORE'] = \"{}/{}/wikidata.sqlite3.db\".format(output_path, temp_folder)\n",
    "\n",
    "#Python variables\n",
    "if cache_path:\n",
    "    store = \"{}/wikidata.sqlite3.db\".format(cache_path)\n",
    "else:\n",
    "    store = \"{}/{}/wikidata.sqlite3.db\".format(output_path, temp_folder)\n",
    "\n",
    "out = \"{}/{}\".format(output_path, output_folder)\n",
    "temp = \"{}/{}\".format(output_path, temp_folder)\n",
    "\n",
    "claims = wiki_root_folder + claims_file\n",
    "labels = wiki_root_folder + label_file\n",
    "aliases = wiki_root_folder + alias_file\n",
    "descriptions = wiki_root_folder + description_file\n",
    "items = wiki_root_folder + item_file\n",
    "quals = wiki_root_folder + qual_file\n",
    "datatypes = wiki_root_folder + property_datatypes_file\n",
    "isa = wiki_root_folder + isa_file\n",
    "p279star = wiki_root_folder + p279star_file\n",
    "\n",
    "# shortcuts to commands\n",
    "kgtk = \"time kgtk --debug\"\n",
    "kypher = \"kgtk query --debug --graph-cache \" + store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the output directory and create the subfolders for the output files and the temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/markmann/Downloads/subset/output: File exists\n",
      "mkdir: /Users/markmann/Downloads/subset/temp.output: File exists\n"
     ]
    }
   ],
   "source": [
    "!cd $output_path\n",
    "!mkdir {out}\n",
    "!mkdir {temp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the output and temp folders before we start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm {out}/*.tsv {out}/*.tsv.gz\n",
    "# !rm {temp}/*.tsv {temp}/*.tsv.gz\n",
    "\n",
    "if delete_database:\n",
    "    !rm {out}/*.tsv {out}/*.tsv.gz\n",
    "    !rm {temp}/*.tsv {temp}/*.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good practice to peek a the files to make sure the column headings are what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gzcat {claims} | head\n",
    "# !zgrep 'Q34508' {claims} #Exists\n",
    "# !zgrep 'Q34508' {labels} #Exists\n",
    "# !zgrep 'Q34508' {aliases} #Exists\n",
    "# !zgrep 'Q34508' {descriptions} #Exists\n",
    "# !{descriptions}\n",
    "\n",
    "!zgrep '\\tQ34508\\t' {claims} -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kypher} -i {claims} \\\n",
    "--match '()-[]->()' \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list of all the items we want to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the items to be removed\n",
    "\n",
    "Use the methods of the below `Remove_Classes` helper to build a list of classes (QNodes) to remove from the Wikidata input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Remove_Classes():\n",
    "    \n",
    "    ######### INIT #########\n",
    "    def __init__(self, temp):\n",
    "        '''temp - should be the path to temp folder'''\n",
    "        self.temp = temp\n",
    "        self.classes_to_remove = set()\n",
    "        self.classes_to_protect = set()\n",
    "        \n",
    "    def count_classes(self, isa, p279star, claims):\n",
    "        '''Finds all classes from isa and p279star files, then counts instances of classes in claims file'''\n",
    "        #Get union of classes from isa and p279star files\n",
    "        self.class_set = self.find_all_classes(isa, p279star)\n",
    "\n",
    "        #Query the claims file, and get a count of each class\n",
    "        self.class_counts = self.get_class_counts(claims)\n",
    "    \n",
    "    def find_all_classes(self, isa, p279star): #called by count_classes()\n",
    "        isa_p = isa.replace('\\\\', '')\n",
    "        p279star_p = p279star.replace('\\\\', '')\n",
    "        class_files = [isa_p, p279star_p]\n",
    "        class_set = set()\n",
    "        for file in class_files:\n",
    "            fd = gzip.open(file, 'rt')\n",
    "            lines=fd.readlines()\n",
    "            count = 0\n",
    "            for line in lines[1:]:\n",
    "                qnode = line.split('\\t')[2].strip()\n",
    "                class_set.add(qnode)\n",
    "        return class_set\n",
    "    \n",
    "    def get_class_counts(self, claims): #called by count_classes()\n",
    "        claims_p = claims.replace('\\\\', '')\n",
    "        fd = gzip.open(claims_p, 'rt')\n",
    "        lines=fd.readlines()\n",
    "\n",
    "        class_counts = dict()\n",
    "        for line in lines[1:]:\n",
    "            n1 = line.split('\\t')[1]\n",
    "            n2 = line.split('\\t')[3]\n",
    "            for n in [n1, n2]:\n",
    "                if n in self.class_set:\n",
    "                    if n not in class_counts: class_counts[n] = 1\n",
    "                    else: class_counts[n] += 1\n",
    "        return class_counts\n",
    "    \n",
    "    ######### METHODS #########                       \n",
    "    def add_instances(self, instances, **kwargs):\n",
    "        '''Identify the set of all classes for a list of instances\n",
    "        instancess - a list of Wikidata instances <list> \n",
    "        kwargs: remove - whether to add to the remove_list or protect_list (True/False)'''\n",
    "        if kwargs:\n",
    "            for qnode in instances:\n",
    "                # !wd u {qnode} > {self.temp}/summary.txt #ipynb\n",
    "                command = f\"wd u {qnode} > {self.temp}/summary.txt\" #py\n",
    "                os.system(command)\n",
    "                fd = open(f'{self.temp}/summary.txt', \"r\")\n",
    "                lines = fd.readlines()\n",
    "                for line in lines:\n",
    "                    if line.split(\":\")[0] in ['instance of (P31)', 'subclass of (P279)']:\n",
    "                        classes_raw = line.split(\":\")[1].split('|')\n",
    "                        for c in classes_raw:\n",
    "                            res = re.findall(r'\\(.*?\\)', c)[0].replace('(','').replace(')','')\n",
    "\n",
    "                            #Add to remove_list or protect_list based on `remove` setting\n",
    "                            if kwargs['remove']: \n",
    "                                self.classes_to_remove.add(res)\n",
    "                            else:\n",
    "                                print('result: ', res)\n",
    "                                self.add_classes_to_protect([res])\n",
    "        else: print('Error: Please specify remove parameter; ex: remove=False, remove=True')\n",
    "        \n",
    "    def add_classes_to_remove(self, **kwargs):\n",
    "        '''Add classes manually to set of classes to remove\n",
    "        kwargs: classes - a list of Wikidata classes <list>\n",
    "        kwargs: size - adds classes with # instances < size'''\n",
    "        if 'classes' in kwargs:\n",
    "            if isinstance(kwargs['classes'], list):\n",
    "                [self.classes_to_remove.add(c) for c in kwargs['classes']]\n",
    "            else: \n",
    "                print('must pass in a list of classes')\n",
    "        if 'size' in kwargs:\n",
    "            for key in self.class_counts.keys():\n",
    "                if self.class_counts[key] <= kwargs['size']: \n",
    "                    self.classes_to_remove.add(key)\n",
    "\n",
    "    def add_classes_to_protect(self, classes):\n",
    "        '''Add classes manually to set of classes to protect\n",
    "        args: classes - list of Wikidata classes'''\n",
    "        for c in list(classes):\n",
    "            # !wdtaxonomy -r {c} -f csv -o {self.temp}/superclass_raw.txt #ipynb\n",
    "            command = f'wdtaxonomy -r {c} -f csv -o {self.temp}/superclass_raw.txt' #py\n",
    "            os.system(command)\n",
    "            fd = open(f'{self.temp}/superclass_raw.txt', \"r\")\n",
    "            lines = fd.readlines()\n",
    "            for line in lines[1:]:\n",
    "                qnode = line.split(',')[1]\n",
    "                if qnode[0].lower() == 'q': \n",
    "                    self.classes_to_protect.add(qnode)\n",
    "                    \n",
    "    def check_conflict(self):\n",
    "        '''Check if any conflicts exist between remove_classes and protect_classses.\n",
    "        If there is an conflict, point out the problematic remove-protect pair. '''\n",
    "        \n",
    "        #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING\n",
    "rc = Remove_Classes(temp)\n",
    "rc.count_classes(isa, p279star, claims)\n",
    "\n",
    "# Test 1a: Remove instances (Q5451712, Fireball)\n",
    "instances = ['Q2468862']\n",
    "rc.add_instances(instances, remove=True)\n",
    "print(len(rc.classes_to_remove)) #>> 1\n",
    "\n",
    "#Test 1b: Remove classes (Q30612, clinical trial)\n",
    "classes = ['Q30612']\n",
    "rc.add_classes_to_remove(classes=classes)\n",
    "print(len(rc.classes_to_remove)) #>> 2\n",
    "\n",
    "# Test 1c: Remove classes with size <= 5\n",
    "rc.add_classes_to_remove(size=5)\n",
    "print(len(rc.classes_to_remove)) #>> 235179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the items to be protected\n",
    "\n",
    "Use the methods of `Remove_Classes` helper to identify classes to preserve (QNode). Also check for conflicts with the list of classes to remove, and display any conflicts to user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  Q15075508\n",
      "90\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "rc = Remove_Classes(temp)\n",
    "\n",
    "#Test 2a: Protect instances (Q15874936, Michelob)\n",
    "instances = ['Q15874936']\n",
    "rc.add_instances(instances, remove=False)\n",
    "print(len(rc.classes_to_protect)) #>> 90\n",
    "\n",
    "#Test 2b-v1: Protect classes (Q44, beer)\n",
    "# classes = ['Q44']\n",
    "# rc.add_classes_to_protect(classes)\n",
    "# print(len(rc.classes_to_protect)) #>> single: size = 31, union_2a: size = 90 (complete overlap)\n",
    "\n",
    "#Test 2b-v2: Protect classes (Q34508, videotape)\n",
    "classes = ['Q34508']\n",
    "rc.add_classes_to_protect(classes)\n",
    "print(len(rc.classes_to_protect)) #>> single: size = 60, union_2a: size = 103 (partial overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any conflicts between `classes_to_remove` and `classes_to_protect` and let the user know in `stdout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect testing metrics\n",
    "\n",
    "For each file, count the number of classes and instances of examples we are testing. \n",
    "We will then remove these classes, and check that the notebook is removing classes as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [('claims', claims), ('labels', labels), ('aliases', aliases), ('descriptions', descriptions)]\n",
    "test_classes = [\"Q281\", \"Q30612\"]\n",
    "\n",
    "# #Test 1a: Count class (Q281, whisky) of given instance (Q5451712, Fireball)\n",
    "#Test 1b: Count class (Q30612, clinical trial)\n",
    "file_counts = {'before': dict(), 'after': dict()}\n",
    "for file in files:\n",
    "    file_counts['before'][file[0]] = {c: 0 for c in test_classes}\n",
    "    fd = gzip.open(file[1].replace('\\\\', ''), 'rt')\n",
    "    lines=fd.readlines()\n",
    "    for line in lines:\n",
    "        for c in test_classes:\n",
    "            if re.search(f\"\\t{c}\\t\", line): \n",
    "                file_counts['before'][file[0]][c] += 1\n",
    "\n",
    "#Test 1c: Count the amount of classes with <= 5 instances\n",
    "size = 5\n",
    "count_classes_small = 0\n",
    "for key in rc.class_counts.keys():\n",
    "    if rc.class_counts[key] <= size: \n",
    "        count_classes_small += 1\n",
    "# count_classes_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counts #1a, 1b\n",
    "count_classes_small #1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose the kypher command to remove the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {isa} | head | col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command, the items to remove will be in file `{temp}/items.remove.tsv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(list(rc.classes_to_remove))\n",
    "# classes = \", \".join(list(map(lambda x: '\"{}\"'.format(x), remove_classes.replace(\" \", \"\").split(\",\"))))\n",
    "\n",
    "# !{kypher}  -i {isa} -i {p279star} -o {temp}/items.remove.tsv.gz \\\n",
    "# --match 'isa: (n1)-[:isa]->(c), P279star: (c)-[]->(class)' \\\n",
    "# --where 'class in [{classes}]' \\\n",
    "# --return 'distinct n1, \"p31_p279star\" as label, class as node2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {temp}/items.remove.tsv.gz | head | col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {temp}/items.remove.tsv.gz | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {temp}/items.remove.tsv.gz | grep 'Q502268\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {temp}/items.remove.tsv.gz | grep 'Q15874936\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the classes of items we will remove, just as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kypher} -i {temp}/items.remove.tsv.gz \\\n",
    "--match '()-[]->(n2)' \\\n",
    "--return 'distinct n2' \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the reduced edges file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the items from the all.tsv and the label, alias and description files\n",
    "We will be left with `reduced` files where the edges do not have the unwanted items. We have to remove them from the node1 and node2 positions, so we need to run the ifnotexists commands twice.\n",
    "\n",
    "Before we start preview the files to see the column headings and check whether they look sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/items.remove.tsv.gz -o {temp}/items.remove.sorted.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {temp}/items.remove.sorted.tsv.gz | head | col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"{claims}\" | head -5 | col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the full set of edges those edges that have a `node1` present in `items.remove.sorted.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifnotexists -i \"{claims}\" -o {temp}/item.edges.reduced.tsv.gz \\\n",
    "--filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "--input-keys node1 \\\n",
    "--filter-keys node1 \\\n",
    "--presorted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the remaining edges, remove those that have a `node2` present in `items.remove.sorted.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/item.edges.reduced.tsv.gz -o {temp}/item.edges.reduced.sorted.tsv.gz \\\n",
    "--columns node2 label node1 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifnotexists -i {temp}/item.edges.reduced.sorted.tsv.gz -o {temp}/item.edges.reduced.2.tsv.gz \\\n",
    "--filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "--input-keys node2 \\\n",
    "--filter-keys node1 \\\n",
    "--presorted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifnotexists -i {labels} -o {temp}/label.edges.reduced.tsv.gz \\\n",
    "--filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "--input-keys node1 \\\n",
    "--filter-keys node1 \\\n",
    "--presorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    cmd = f\"kgtk sort2 -i {temp}/label.{lang}.edges.reduced.tsv.gz -o {out}/labels.{lang}.tsv.gz\" \n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file with the aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifnotexists -i {aliases} -o {temp}/alias.edges.reduced.tsv.gz \\\n",
    "--filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "--input-keys node1 \\\n",
    "--filter-keys node1 \\\n",
    "--presorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    cmd = f\"kgtk --debug ifnotexists -i {wiki_root_folder}aliases.{lang}.tsv.gz \\\n",
    "    -o {temp}/alias.{lang}.edges.reduced.tsv.gz \\\n",
    "    --filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "    --input-keys node1 \\\n",
    "    --filter-keys node1 \\\n",
    "    --presorted\"\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    cmd = f\"kgtk sort2 -i {temp}/alias.{lang}.edges.reduced.tsv.gz -o {out}/aliases.{lang}.tsv.gz\" \n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file with the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifnotexists -i {descriptions} -o {temp}/description.edges.reduced.tsv.gz \\\n",
    "--filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "--input-keys node1 \\\n",
    "--filter-keys node1 \\\n",
    "--presorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    cmd = f\"kgtk --debug ifnotexists -i {wiki_root_folder}descriptions.{lang}.tsv.gz \\\n",
    "    -o {temp}/description.{lang}.edges.reduced.tsv.gz \\\n",
    "    --filter-on {temp}/items.remove.sorted.tsv.gz \\\n",
    "    --input-keys node1 \\\n",
    "    --filter-keys node1 \\\n",
    "    --presorted\"\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    cmd = f\"kgtk sort2 -i {temp}/description.{lang}.edges.reduced.tsv.gz -o {out}/descriptions.{lang}.tsv.gz\" \n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the output files for claims, labels, aliases and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/item.edges.reduced.2.tsv.gz -o {out}/claims.tsv.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/label.edges.reduced.tsv.gz -o {out}/labels.en.tsv.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/alias.edges.reduced.tsv.gz -o {out}/aliases.en.tsv.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk sort2 -i {temp}/description.edges.reduced.tsv.gz -o {out}/descriptions.en.tsv.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the reduced qualifiers file\n",
    "We do this by finding all the ids of the reduced edges file, and then selecting out from `qual.tsv`\n",
    "\n",
    "We need to join by id, so we need to sort both files by id, node1, label, node2:\n",
    "\n",
    "- `{quals}` \n",
    "- `{out}/claims.tsv.gz` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"{quals}\" | head | column -t -s $'\\t' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `ifexists` to select out the quals for the edges in `{out}/wikidataos.qual.tsv.gz`. Note that we use `node1` in the qualifier file, matching to `id` in the `wikidataos.all.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$kgtk ifexists -i \"{quals}\" -o {out}/qualifiers.tsv.gz \\\n",
    "--filter-on {out}/claims.tsv.gz \\\n",
    "--input-keys node1 \\\n",
    "--filter-keys id \\\n",
    "--presorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the final output for qualifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {out}/qualifiers.tsv.gz | head | col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgtk_path = \"/Users/pedroszekely/Documents/GitHub/kgtk\"\n",
    "os.environ[\"EXAMPLES_DIR\"] = kgtk_path + \"/examples\"\n",
    "os.environ[\"USECASE_DIR\"] = kgtk_path + \"/use-cases\"\n",
    "os.environ[\"TEMP\"] = temp\n",
    "os.environ[\"OUT\"] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"$TEMP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"$OUT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $kgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk cat \\\n",
    "-i \"$OUT\"/aliases.en.tsv.gz \\\n",
    "-i \"$OUT\"/descriptions.en.tsv.gz \\\n",
    "-i \"$OUT\"/qualifiers.tsv.gz \\\n",
    "-i \"$OUT\"/claims.tsv.gz \\\n",
    "-i \"$OUT\"/labels.en.tsv.gz \\\n",
    "-i \"$OUT\"/metadata.property.datatypes.tsv.gz \\\n",
    "-i \"$OUT\"/metadata.types.tsv.gz \\\n",
    "-o \"$OUT\"/all.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {os.environ[\"EXAMPLES_DIR\"] + \"/partition-wikidata.ipynb\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.execute_notebook(\n",
    "    os.environ[\"EXAMPLES_DIR\"] + \"/partition-wikidata.ipynb\",\n",
    "    os.environ[\"TEMP\"] + \"/partition-wikidata.out.ipynb\",\n",
    "    parameters=dict(\n",
    "        wikidata_input_path = os.environ[\"OUT\"] + \"/all.tsv.gz\",\n",
    "        wikidata_parts_path = os.environ[\"OUT\"] + \"/parts\",\n",
    "        temp_folder_path = os.environ[\"OUT\"] + \"/parts/temp\",\n",
    "        sort_extras = \"--buffer-size 30% --temporary-directory $OUT/parts/temp\",\n",
    "        verbose = False\n",
    "    )\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.execute_notebook(\n",
    "    os.environ[\"USECASE_DIR\"] + \"/Wikidata Useful Files.ipynb\",\n",
    "    os.environ[\"TEMP\"] + \"/Wikidata Useful Files Out.ipynb\",\n",
    "    parameters=dict(\n",
    "        output_path = os.environ[\"OUT\"],\n",
    "        output_folder = \"useful_files\",\n",
    "        temp_folder = \"temp.useful_files\",\n",
    "        wiki_root_folder = os.environ[\"OUT\"] + \"/parts/\",\n",
    "        cache_path = os.environ[\"OUT\"] + \"/temp.useful_files\",\n",
    "        languages = 'en',\n",
    "        compute_pagerank = True,\n",
    "        delete_database = False\n",
    "    )\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kypher} -i {out}/claims.tsv.gz \\\n",
    "--match '(n1:Q368441)-[l]->(n2)' \\\n",
    "--limit 10 \\\n",
    "| col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kypher} -i {out}/claims.tsv.gz \\\n",
    "--match '(n1:P131)-[l]->(n2)' \\\n",
    "--limit 10 \\\n",
    "| col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the derived files using the `Wikidata Useful Files` Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute `claims.wikibase-item.tsv.gz` which would be computed by the Wikidata partitioner, but we are not using it here yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < \"{datatypes}\" | head | col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kypher} -i {out}/claims.tsv.gz -i \"{datatypes}\" -o {out}/claims.wikibase-item.tsv.gz \\\n",
    "--match 'claims: (n1)-[l {label: p}]->(n2), datatypes: (p)-[:datatype]->(:`wikibase-item`)' \\\n",
    "--return 'l as id, n1 as node1, p as label, n2 as node2' \\\n",
    "--order-by 'l' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the derived files we use papermill to run the `Wikidata Useful Files` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.execute_notebook(\n",
    "    notebooks_folder + useful_files_notebook,\n",
    "    temp + \"/useful_files_notebook_output.ipynb\",\n",
    "    parameters=dict(\n",
    "        output_path=output_path,\n",
    "        output_folder=output_folder,\n",
    "        temp_folder=temp_folder,\n",
    "        wiki_root_folder=wiki_root_folder,\n",
    "        claims_file=\"claims.tsv.gz\",\n",
    "        label_file=\"labels.en.tsv.gz\",\n",
    "        alias_file=\"aliases.en.tsv.gz\",\n",
    "        description_file=\"descriptions.en.tsv.gz\",\n",
    "        item_file=\"claims.wikibase-item.tsv.gz\",\n",
    "        cache_path=cache_path,\n",
    "        delete_database=delete_database,\n",
    "        compute_pagerank=compute_pagerank\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the columns so we know how to construct the kypher query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {out}/*wikidataos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat < {out}/wikidataos.all.tsv.gz | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges file must contain edges for properties, this is not the case on 2020-11-10`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{kgtk} -i \"{claims}\" \\\n",
    "--match '(:P10)-[l]->(n2)' \\\n",
    "--limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate files to get the `all` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lad = []\n",
    "if 'en' not in languages:\n",
    "    languages.append('en')\n",
    "for lang in languages:\n",
    "    lad.append(f\"{out}/labels.{lang}.tsv.gz\")\n",
    "    lad.append(f\"{out}/aliases.{lang}.tsv.gz\")\n",
    "    lad.append(f\"{out}/descriptions.{lang}.tsv.gz\")\n",
    "lad_file_list = \" \".join(lad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk cat -i {out}/claims.tsv.gz \\\n",
    "{lad_file_list} \\\n",
    "{out}/qualifiers.tsv.gz \\\n",
    "{out}/metadata.pagerank.undirected.tsv.gz \\\n",
    "{out}/metadata.pagerank.directed.tsv.gz \\\n",
    "{out}/metadata.in_degree.tsv.gz \\\n",
    "{out}/metadata.out_degree.tsv.gz \\\n",
    "-o {out}/wikidataos.all.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenate files to get the `all for triples` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk cat -i $OUT/wikidataos.all.tsv.gz \\\n",
    "$OUT/derived.P31.tsv.gz \\\n",
    "$OUT/derived.P279.tsv.gz \\\n",
    "$OUT/derived.isa.tsv.gz \\\n",
    "$OUT/derived.P279star.tsv.gz \\\n",
    "-o $OUT/wikidataos.all.for.triples.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out `novalue`, `somevalue` and `P9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk filter -i $OUT/wikidataos.all.for.triples.tsv.gz \\\n",
    "    -o $OUT/wikidataos.all.for.triples.filtered.tsv.gz \\\n",
    "    -p ';;somevalue,novalue,P9' --invert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ids for any edge with missing id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk add-id -i $OUT/wikidataos.all.for.triples.filtered.tsv.gz \\\n",
    "-o $OUT/wikidataos.all.for.triples.filtered.id.tsv.gz \\\n",
    "--id-style wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kgtk sort2 -i $OUT/wikidataos.all.for.triples.filtered.id.tsv.gz \\\n",
    "-o $OUT/wikidataos.all.for.triples.filtered.id.sorted.tsv.gz \n",
    "-c id"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "kgtk-subset",
   "language": "python",
   "name": "kgtk-subset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
